# multilingual-generalization-papers
This is a list of papers I'm taking a look at that pertain to improving the performance of multilingual LMs. Last updated: 5/29/22


## Prompting Multilingual LMs


- Few-shot Learning with Multilingual Language Models (https://arxiv.org/pdf/2112.10668.pdf)
- Language Models are Few-shot Multilingual Learners (https://arxiv.org/pdf/2109.07684.pdf)
- Polyglot Prompt: Multilingual Multitask PrompTraining (https://arxiv.org/pdf/2204.14264.pdf)
- Discrete and Soft Prompting for Multilingual Models (https://arxiv.org/pdf/2109.03630.pdf)
- Zero-shot Cross-lingual Transfer of Prompt-based Tuning with a Unified Multilingual Prompt (https://arxiv.org/pdf/2202.11451.pdf)
- Enhancing Cross-lingual Prompting with Two-level Augmentation (https://openreview.net/pdf?id=ByVyEk50138)
- MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators(https://openreview.net/pdf?id=Uhnw1smvKt)
- A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters (https://aclanthology.org/2021.acl-long.447.pdf)



## Extending / Adapting Multilingual LMs to new languages

- Extending Multilingual BERT to Low-Resource Languages (https://aclanthology.org/2020.findings-emnlp.240.pdf)
- Multilingual Language Model Adaptive Fine-Tuning: A Study On African Languages (https://arxiv.org/pdf/2204.06487.pdf)
- MAD-X: An Adapter-Based Framework for
Multi-Task Cross-Lingual Transfer (https://aclanthology.org/2020.emnlp-main.617.pdf)
- Continual Learning in Multilingual NMT via Language-Specific
Embeddings (https://aclanthology.org/2021.wmt-1.62.pdf)
- UNKs Everywhere:
Adapting Multilingual Language Models to New Scripts (https://aclanthology.org/2021.emnlp-main.800.pdf)
- How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models (https://arxiv.org/pdf/2012.15613.pdf)
- AdapterFusion:
Non-Destructive Task Composition for Transfer Learning (https://aclanthology.org/2021.eacl-main.39.pdf)
- When Being Unseen from mBERT is just the Beginning:
Handling New Languages With Multilingual Language Models (https://aclanthology.org/2021.naacl-main.38.pdf)
- Adapting BigScience Multilingual Model to Unseen Languages (https://arxiv.org/pdf/2204.04873.pdf)
- Lifting the Curse of Multilinguality by Pre-training Modular Transformers (https://arxiv.org/pdf/2205.06266.pdf)


## Multilingual Corpora
- Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets (https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00447/109285/Quality-at-a-Glance-An-Audit-of-Web-Crawled)
- Language Contamination Explains the Cross-lingual Capabilities of English Pretrained Language Models (https://blvns.github.io/papers/blevins2022language.pdf)

